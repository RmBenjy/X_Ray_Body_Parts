{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# Ignore Python's warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ignore TensorFlow's warnings\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'C:\\\\Users\\\\reda-\\\\Desktop\\\\X_Ray Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(base_dir,'train')\n",
    "test_dir = os.path.join(base_dir,'test')\n",
    "valid_dir = os.path.join(base_dir,'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224\n",
    "input_shape = (img_width, img_height, 1) \n",
    "num_classes = 13  # Number of subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1.0/255.0,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip=True\n",
    "    \n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1543 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 221 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load testing data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = 32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 439 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load testing data\n",
    "valid_generator = test_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = 32,\n",
    "    class_mode='categorical',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(224, 224, 3), groups=1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu', groups=1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu', groups=1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 38s 751ms/step - loss: 1.9004 - accuracy: 0.4666 - val_loss: 1.2709 - val_accuracy: 0.6036\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 36s 727ms/step - loss: 1.2040 - accuracy: 0.6487 - val_loss: 0.9085 - val_accuracy: 0.7312\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 36s 728ms/step - loss: 1.0427 - accuracy: 0.6896 - val_loss: 0.8454 - val_accuracy: 0.7631\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 38s 764ms/step - loss: 0.9247 - accuracy: 0.7278 - val_loss: 0.7511 - val_accuracy: 0.7904\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 38s 767ms/step - loss: 0.8812 - accuracy: 0.7440 - val_loss: 0.7837 - val_accuracy: 0.7927\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 59s 1s/step - loss: 0.7451 - accuracy: 0.7764 - val_loss: 0.7792 - val_accuracy: 0.7631\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 58s 1s/step - loss: 0.7094 - accuracy: 0.7907 - val_loss: 0.7503 - val_accuracy: 0.7745\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 37s 751ms/step - loss: 0.6647 - accuracy: 0.7997 - val_loss: 0.6879 - val_accuracy: 0.7927\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 38s 775ms/step - loss: 0.6436 - accuracy: 0.8062 - val_loss: 0.7369 - val_accuracy: 0.7836\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 41s 832ms/step - loss: 0.6129 - accuracy: 0.8205 - val_loss: 0.6659 - val_accuracy: 0.8018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e24de5b9d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit(train_generator, epochs=10, validation_data=valid_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 128ms/step - loss: 0.7342 - accuracy: 0.7602\n",
      "Accuracy on test set: 76.02%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(test_generator)\n",
    "print(\"Accuracy on test set: {:.2f}%\".format(accuracy[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.preprocessing import image as image_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 112, 112, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 56, 56, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 28, 28, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100352)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               12845184  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 13)                1677      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12940109 (49.36 MB)\n",
      "Trainable params: 12940109 (49.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"bassin\",\"cheville\",\"colonne_vertebrale\", \"cou\", \"coude\", \"epaule\", \"genou\", \"jambe\", \"main\", \"pectoral poitrine\", \"pied\", \"poignet\", \"tete\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_with_prediction(image_path, predicted_body_part):\n",
    "    display(Image(filename=image_path, width=600))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 144ms/step\n",
      "Chemin : C:\\Users\\reda-\\Desktop\\X_Ray Images\\test\\colonne_vertebrale\\1-2-826-0-1-3680043-8-498-59113893969561724980199447804671753951-c_png_jpg.rf.ce19b992402e72e0d70a9ef9f1014243.jpg\n",
      "Vraie étiquette : colonne_vertebrale\n",
      "Prédiction du modèle : colonne_vertebrale\n",
      "Confiance du modèle : 99.02%\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/jpeg": {
       "width": 600
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Faites des prédictions pour chaque lot d'images\n",
    "predictions = model.predict(test_generator)\n",
    "\n",
    "# Obtenez les indices de classe prédits pour chaque image\n",
    "predicted_class_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Obtenez les noms de classe correspondant aux indices\n",
    "predicted_class_labels = [class_labels[i] for i in predicted_class_indices]\n",
    "\n",
    "# Obtenez les pourcentages de confiance du modèle pour chaque prédiction\n",
    "confidence_percentages = np.max(predictions, axis=1) * 100\n",
    "\n",
    "# Créez un dataframe pour stocker les informations\n",
    "df_results = pd.DataFrame({\n",
    "    'Image': test_generator.filepaths,\n",
    "    'Vraie étiquette': [class_labels[i] for i in test_generator.classes],\n",
    "    'Prédiction du modèle': predicted_class_labels,\n",
    "    'Confiance (%)': confidence_percentages\n",
    "})\n",
    "\n",
    "# Sélectionnez une seule image aléatoire\n",
    "random_row = df_results.sample()\n",
    "\n",
    "# Affichez l'image avec ses prédictions et pourcentage de confiance\n",
    "image_path = random_row['Image'].values[0]\n",
    "predicted_label = random_row['Prédiction du modèle'].values[0]\n",
    "confidence_percentage = random_row['Confiance (%)'].values[0]\n",
    "\n",
    "print(f\"Chemin : {image_path}\")\n",
    "print(f\"Vraie étiquette : {random_row['Vraie étiquette'].values[0]}\")\n",
    "print(f\"Prédiction du modèle : {predicted_label}\")\n",
    "print(f\"Confiance du modèle : {confidence_percentage:.2f}%\\n\")\n",
    "show_image_with_prediction(image_path, predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
