{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Ignore Python's warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ignore TensorFlow's warnings\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'C:\\\\Users\\\\reda-\\\\Desktop\\\\X_Ray Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(base_dir,'train')\n",
    "test_dir = os.path.join(base_dir,'test')\n",
    "valid_dir = os.path.join(base_dir,'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224\n",
    "input_shape = (img_width, img_height, 1) \n",
    "num_classes = 13  # Number of subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1.0/255.0,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip=True\n",
    "    \n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1543 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 221 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load testing data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = 32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 439 images belonging to 13 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load testing data\n",
    "valid_generator = test_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = 32,\n",
    "    class_mode='categorical',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(224, 224, 3), groups=1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu', groups=1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu', groups=1))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "49/49 [==============================] - 40s 776ms/step - loss: 1.8458 - accuracy: 0.4750 - val_loss: 1.3727 - val_accuracy: 0.5809\n",
      "Epoch 2/25\n",
      "49/49 [==============================] - 52s 1s/step - loss: 1.2569 - accuracy: 0.6209 - val_loss: 0.9955 - val_accuracy: 0.7221\n",
      "Epoch 3/25\n",
      "49/49 [==============================] - 64s 1s/step - loss: 1.0652 - accuracy: 0.6773 - val_loss: 0.9379 - val_accuracy: 0.7517\n",
      "Epoch 4/25\n",
      "49/49 [==============================] - 64s 1s/step - loss: 0.9803 - accuracy: 0.7006 - val_loss: 0.8238 - val_accuracy: 0.7494\n",
      "Epoch 5/25\n",
      "49/49 [==============================] - 59s 1s/step - loss: 0.9252 - accuracy: 0.7181 - val_loss: 0.7863 - val_accuracy: 0.7790\n",
      "Epoch 6/25\n",
      "49/49 [==============================] - 62s 1s/step - loss: 0.7802 - accuracy: 0.7641 - val_loss: 0.7721 - val_accuracy: 0.7813\n",
      "Epoch 7/25\n",
      "49/49 [==============================] - 63s 1s/step - loss: 0.7027 - accuracy: 0.7816 - val_loss: 0.7172 - val_accuracy: 0.8064\n",
      "Epoch 8/25\n",
      "49/49 [==============================] - 63s 1s/step - loss: 0.6262 - accuracy: 0.8023 - val_loss: 0.7867 - val_accuracy: 0.7677\n",
      "Epoch 9/25\n",
      "49/49 [==============================] - 62s 1s/step - loss: 0.6045 - accuracy: 0.8218 - val_loss: 0.7611 - val_accuracy: 0.7722\n",
      "Epoch 10/25\n",
      "49/49 [==============================] - 62s 1s/step - loss: 0.5547 - accuracy: 0.8270 - val_loss: 0.7339 - val_accuracy: 0.8109\n",
      "Epoch 11/25\n",
      "49/49 [==============================] - 73s 1s/step - loss: 0.4987 - accuracy: 0.8412 - val_loss: 0.6944 - val_accuracy: 0.8132\n",
      "Epoch 12/25\n",
      "49/49 [==============================] - 73s 1s/step - loss: 0.5022 - accuracy: 0.8425 - val_loss: 0.7741 - val_accuracy: 0.7995\n",
      "Epoch 13/25\n",
      "49/49 [==============================] - 75s 2s/step - loss: 0.4318 - accuracy: 0.8678 - val_loss: 0.7104 - val_accuracy: 0.8178\n",
      "Epoch 14/25\n",
      "49/49 [==============================] - 74s 2s/step - loss: 0.4323 - accuracy: 0.8671 - val_loss: 0.6986 - val_accuracy: 0.8269\n",
      "Epoch 15/25\n",
      "49/49 [==============================] - 75s 2s/step - loss: 0.3692 - accuracy: 0.8814 - val_loss: 0.6869 - val_accuracy: 0.8246\n",
      "Epoch 16/25\n",
      "49/49 [==============================] - 74s 2s/step - loss: 0.3988 - accuracy: 0.8859 - val_loss: 0.6506 - val_accuracy: 0.8383\n",
      "Epoch 17/25\n",
      "49/49 [==============================] - 75s 2s/step - loss: 0.2920 - accuracy: 0.9112 - val_loss: 0.6438 - val_accuracy: 0.8405\n",
      "Epoch 18/25\n",
      "49/49 [==============================] - 64s 1s/step - loss: 0.2916 - accuracy: 0.9177 - val_loss: 0.7755 - val_accuracy: 0.8200\n",
      "Epoch 19/25\n",
      "49/49 [==============================] - 74s 1s/step - loss: 0.2706 - accuracy: 0.9203 - val_loss: 0.7535 - val_accuracy: 0.8428\n",
      "Epoch 20/25\n",
      "49/49 [==============================] - 74s 2s/step - loss: 0.2793 - accuracy: 0.9170 - val_loss: 0.7616 - val_accuracy: 0.8269\n",
      "Epoch 21/25\n",
      "49/49 [==============================] - 76s 2s/step - loss: 0.2317 - accuracy: 0.9294 - val_loss: 0.8005 - val_accuracy: 0.8178\n",
      "Epoch 22/25\n",
      "49/49 [==============================] - 75s 2s/step - loss: 0.2145 - accuracy: 0.9300 - val_loss: 0.7644 - val_accuracy: 0.8200\n",
      "Epoch 23/25\n",
      "49/49 [==============================] - 76s 2s/step - loss: 0.2114 - accuracy: 0.9352 - val_loss: 0.7945 - val_accuracy: 0.8269\n",
      "Epoch 24/25\n",
      "49/49 [==============================] - 76s 2s/step - loss: 0.1901 - accuracy: 0.9371 - val_loss: 0.7159 - val_accuracy: 0.8451\n",
      "Epoch 25/25\n",
      "49/49 [==============================] - 71s 1s/step - loss: 0.1655 - accuracy: 0.9475 - val_loss: 0.7793 - val_accuracy: 0.8269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dba70ba850>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit(train_generator, epochs=25, validation_data=valid_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 192ms/step - loss: 0.9705 - accuracy: 0.7964\n",
      "Accuracy on test set: 79.64%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(test_generator)\n",
    "print(\"Accuracy on test set: {:.2f}%\".format(accuracy[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 224, 224, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 112, 112, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 56, 56, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 28, 28, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100352)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               12845184  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 13)                1677      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12940109 (49.36 MB)\n",
      "Trainable params: 12940109 (49.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"pelvis\", \"ankle\", \"spine\", \"neck\", \"elbow\", \"shoulder\", \"knee\", \"leg\", \"hand\", \"pectoral chest\", \"foot\", \"wrist\", \"head\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_with_prediction(image_path, predicted_body_part):\n",
    "    display(Image(filename=image_path, width=600))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 211ms/step\n",
      "Path : C:\\Users\\reda-\\Desktop\\X_Ray Images\\test\\pectoral poitrine\\1-2-826-0-1-3680043-8-498-49569514187101813908206599366725265813-c_png_jpg.rf.da6042d889d518aa8b35e8523e7169e5.jpg\n",
      "Real Label : pectoral chest\n",
      "Model prediction : pectoral chest\n",
      "Model confidence : 100.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/jpeg": {
       "width": 600
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions for each batch of images\n",
    "predictions = model.predict(test_generator)\n",
    "\n",
    "# Get the predicted class indices for each image\n",
    "predicted_class_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get class names matching indices\n",
    "predicted_class_labels = [class_labels[i] for i in predicted_class_indices]\n",
    "\n",
    "# Get model confidence percentages for each prediction\n",
    "confidence_percentages = np.max(predictions, axis=1) * 100\n",
    "\n",
    "# Create a dataframe to store information\n",
    "df_results = pd.DataFrame({\n",
    "    'Image': test_generator.filepaths,\n",
    "    'Real Label': [class_labels[i] for i in test_generator.classes],\n",
    "    'Model prediction': predicted_class_labels,\n",
    "    'Confidence (%)': confidence_percentages\n",
    "})\n",
    "\n",
    "# Select a single random image\n",
    "random_row = df_results.sample()\n",
    "\n",
    "# Display the image with its predictions and confidence percentage\n",
    "image_path = random_row['Image'].values[0]\n",
    "predicted_label = random_row['Model prediction'].values[0]\n",
    "confidence_percentage = random_row['Confidence (%)'].values[0]\n",
    "\n",
    "print(f\"Path : {image_path}\")\n",
    "print(f\"Real Label : {random_row['Real Label'].values[0]}\")\n",
    "print(f\"Model prediction : {predicted_label}\")\n",
    "print(f\"Model confidence : {confidence_percentage:.2f}%\\n\")\n",
    "show_image_with_prediction(image_path, predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
